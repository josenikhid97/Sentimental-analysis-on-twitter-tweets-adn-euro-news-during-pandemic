{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhkhIRtFigCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This modeling code is based on the tutorial script here:\n",
        "# https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26X9S4BdasG2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "71e2b125-6cff-46a4-ce98-5026538dce68"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JajIyFa0Ac-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0b57745-4b99-430c-a37a-38c75044d6bf"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LlRmAt1Aeja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "21d76ed5-dd9e-458d-cbec-0f7e5f643e29"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvXQHVT4A2Av",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "d9acbb68-62a1-472c-db18-9bc023892617"
      },
      "source": [
        "!pip install transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3oHSOpVYkdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_data = pd.read_csv('/content/drive/My Drive/DS_week8/BERT/sentiment_12000.csv')\n",
        "data_train, data_test = train_test_split(tweets_data, test_size=0.2, random_state=0)\n",
        "\n",
        "# Reset index\n",
        "data_train = data_train.set_index(pd.Index(range(0,len(data_train))))\n",
        "data_test = data_test.set_index(pd.Index(range(0,len(data_test))))\n",
        "\n",
        "# Save the dataset\n",
        "data_train.to_csv('Train.csv',index=False)\n",
        "data_test.to_csv('Test.csv',index=False)"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbcieC9nA5Hp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6feca2ff-0fa8-4992-f819-e96017bf81e2"
      },
      "source": [
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/Train.csv\")\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>has_media</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>img_urls</th>\n",
              "      <th>is_replied</th>\n",
              "      <th>is_reply_to</th>\n",
              "      <th>likes</th>\n",
              "      <th>links</th>\n",
              "      <th>parent_tweet_id</th>\n",
              "      <th>replies</th>\n",
              "      <th>reply_to_users</th>\n",
              "      <th>retweets</th>\n",
              "      <th>screen_name</th>\n",
              "      <th>text</th>\n",
              "      <th>text_html</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>timestamp_epochs</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_url</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>video_url</th>\n",
              "      <th>hashtags/0</th>\n",
              "      <th>links/0</th>\n",
              "      <th>hashtags/1</th>\n",
              "      <th>hashtags/2</th>\n",
              "      <th>reply_to_users/0/screen_name</th>\n",
              "      <th>reply_to_users/0/user_id</th>\n",
              "      <th>img_urls/0</th>\n",
              "      <th>hashtags/3</th>\n",
              "      <th>hashtags/4</th>\n",
              "      <th>hashtags/5</th>\n",
              "      <th>hashtags/6</th>\n",
              "      <th>img_urls/1</th>\n",
              "      <th>img_urls/2</th>\n",
              "      <th>img_urls/3</th>\n",
              "      <th>reply_to_users/1/screen_name</th>\n",
              "      <th>reply_to_users/1/user_id</th>\n",
              "      <th>reply_to_users/2/screen_name</th>\n",
              "      <th>reply_to_users/2/user_id</th>\n",
              "      <th>hashtags/7</th>\n",
              "      <th>hashtags/8</th>\n",
              "      <th>hashtags/9</th>\n",
              "      <th>links/1</th>\n",
              "      <th>hashtags/10</th>\n",
              "      <th>hashtags/11</th>\n",
              "      <th>hashtags/12</th>\n",
              "      <th>hashtags/13</th>\n",
              "      <th>hashtags/14</th>\n",
              "      <th>hashtags/15</th>\n",
              "      <th>hashtags/16</th>\n",
              "      <th>hashtags/17</th>\n",
              "      <th>hashtags/18</th>\n",
              "      <th>hashtags/19</th>\n",
              "      <th>hashtags/20</th>\n",
              "      <th>hashtags/21</th>\n",
              "      <th>hashtags/22</th>\n",
              "      <th>hashtags/23</th>\n",
              "      <th>links/2</th>\n",
              "      <th>links/3</th>\n",
              "      <th>links/4</th>\n",
              "      <th>hashtags/24</th>\n",
              "      <th>hashtags/25</th>\n",
              "      <th>hashtags/26</th>\n",
              "      <th>hashtags/27</th>\n",
              "      <th>hashtags/28</th>\n",
              "      <th>hashtags/29</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>sentences_tone</th>\n",
              "      <th>document_tones</th>\n",
              "      <th>joy</th>\n",
              "      <th>confident</th>\n",
              "      <th>analytical</th>\n",
              "      <th>sadness</th>\n",
              "      <th>tentative</th>\n",
              "      <th>fear</th>\n",
              "      <th>anger</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>False</td>\n",
              "      <td>[\"COVID19\"]</td>\n",
              "      <td>[]</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>6</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>icedoc61</td>\n",
              "      <td>Today I attended my friend’s funeral via zoom....</td>\n",
              "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
              "      <td>2020-05-09T23:53:08</td>\n",
              "      <td>1.589068e+09</td>\n",
              "      <td>1.259270e+18</td>\n",
              "      <td>/icedoc61/status/1259270210009411584</td>\n",
              "      <td>9.201643e+17</td>\n",
              "      <td>Sotirios Kassapidis</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Today I attended my friend’s funeral via zoom....</td>\n",
              "      <td>[{'sentence_id': 0, 'text': 'Today I attended ...</td>\n",
              "      <td>[{'score': 0.937736, 'tone_id': 'sadness', 'to...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10WallStreet</td>\n",
              "      <td>I Agree with #RandPaul. #Covid19 is a Nasty Fl...</td>\n",
              "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
              "      <td>2020-05-12T23:59:54</td>\n",
              "      <td>1.589328e+09</td>\n",
              "      <td>1.260359e+18</td>\n",
              "      <td>/10WallStreet/status/1260359074157395972</td>\n",
              "      <td>7.830467e+08</td>\n",
              "      <td>@10WallStreet</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RandPaul</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Covid19</td>\n",
              "      <td>CoronaVirus</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>msnbc</td>\n",
              "      <td>foxnews</td>\n",
              "      <td>nytimes</td>\n",
              "      <td>cnn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>wsj</td>\n",
              "      <td>cnbc</td>\n",
              "      <td>politico</td>\n",
              "      <td>NaN</td>\n",
              "      <td>huffpost</td>\n",
              "      <td>drudge</td>\n",
              "      <td>bbc</td>\n",
              "      <td>gop</td>\n",
              "      <td>npr</td>\n",
              "      <td>dailykos</td>\n",
              "      <td>wapo</td>\n",
              "      <td>nbc</td>\n",
              "      <td>cbs</td>\n",
              "      <td>ap</td>\n",
              "      <td>slate</td>\n",
              "      <td>aarp</td>\n",
              "      <td>fauci</td>\n",
              "      <td>cdc</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I Agree with #RandPaul. #Covid19 is a Nasty Fl...</td>\n",
              "      <td>[{'sentence_id': 0, 'text': 'I Agree with #Ran...</td>\n",
              "      <td>[{'score': 0.865841, 'tone_id': 'tentative', '...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>830</th>\n",
              "      <td>False</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>[\"https://twitter.com/KillSection/status/12563...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>MagaNosferatuAK</td>\n",
              "      <td>Holy shit the 5G coronavirus man https://twitt...</td>\n",
              "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
              "      <td>2020-05-01T23:45:05</td>\n",
              "      <td>1.588377e+09</td>\n",
              "      <td>1.256369e+18</td>\n",
              "      <td>/MagaNosferatuAK/status/1256369081042829312</td>\n",
              "      <td>3.614417e+09</td>\n",
              "      <td>(not political)Drew</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Holy shit the 5G coronavirus man</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'score': 0.751885, 'tone_id': 'anger', 'tone...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989</th>\n",
              "      <td>True</td>\n",
              "      <td>[\"CoronaVirus\",\"COVIDー19\",\"COVID19\",\"CoronaVir...</td>\n",
              "      <td>[\"https://pbs.twimg.com/media/EV4ZXw4X0AEGa1S....</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>[\"https://buff.ly/2wQ2j3i\"]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>jamesvgingerich</td>\n",
              "      <td>A New Statistic Reveals Why America’s #CoronaV...</td>\n",
              "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
              "      <td>2020-04-29T23:45:00</td>\n",
              "      <td>1.588204e+09</td>\n",
              "      <td>1.255644e+18</td>\n",
              "      <td>/jamesvgingerich/status/1255644281953378304</td>\n",
              "      <td>1.522160e+07</td>\n",
              "      <td>James Gingerich, @Expeflow #WorkEasier #RPA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A New Statistic Reveals Why America’s #CoronaV...</td>\n",
              "      <td>[{'sentence_id': 0, 'text': 'A New Statistic R...</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>True</td>\n",
              "      <td>[\"Pence\",\"IDIOT\",\"COVID19\",\"TRUMPCLONEpic\"]</td>\n",
              "      <td>[\"https://pbs.twimg.com/media/EWuoBxvXsAIQrlX....</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>jwhitby1</td>\n",
              "      <td>WHAT??? !!! WHY ????? !!!!!\\nDon't do what #Pe...</td>\n",
              "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
              "      <td>2020-04-28T23:44:54</td>\n",
              "      <td>1.588117e+09</td>\n",
              "      <td>1.255282e+18</td>\n",
              "      <td>/jwhitby1/status/1255281871627472896</td>\n",
              "      <td>2.031625e+07</td>\n",
              "      <td>Joyce Whitby</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WHAT??? !!! WHY ????? !!!!!\\nDon't do what #Pe...</td>\n",
              "      <td>[{'sentence_id': 0, 'text': 'WHAT??? !!! WHY ?...</td>\n",
              "      <td>[{'score': 0.58544, 'tone_id': 'anger', 'tone_...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1320</th>\n",
              "      <td>False</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"http://chasecarbon.com/?id=2153803&amp;240803a7\"]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>LondonUwf</td>\n",
              "      <td>independent: 'Nurse in anguished pep talk phot...</td>\n",
              "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
              "      <td>2020-05-01T23:40:04</td>\n",
              "      <td>1.588376e+09</td>\n",
              "      <td>1.256368e+18</td>\n",
              "      <td>/LondonUwf/status/1256367815650656262</td>\n",
              "      <td>1.242431e+18</td>\n",
              "      <td>LondonUWF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>independent: 'Nurse in anguished pep talk phot...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'score': 0.855572, 'tone_id': 'analytical', ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1122</th>\n",
              "      <td>False</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"https://twitter.com/MariaBonanno9/status/125...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gregraven</td>\n",
              "      <td>These arrests are needed to refilled the jails...</td>\n",
              "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
              "      <td>2020-05-02T23:39:29</td>\n",
              "      <td>1.588463e+09</td>\n",
              "      <td>1.256730e+18</td>\n",
              "      <td>/gregraven/status/1256730059294953473</td>\n",
              "      <td>1.821550e+07</td>\n",
              "      <td>Greg Raven</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>These arrests are needed to refilled the jails...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'score': 0.532316, 'tone_id': 'anger', 'tone...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NichelleMitche1</td>\n",
              "      <td>Trump administration to require race data in c...</td>\n",
              "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
              "      <td>2020-06-04T23:55:59</td>\n",
              "      <td>1.591315e+09</td>\n",
              "      <td>1.268693e+18</td>\n",
              "      <td>/NichelleMitche1/status/1268693009052426240</td>\n",
              "      <td>5.736511e+08</td>\n",
              "      <td>Nichelle Mitchem</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://abcn.ws/3gWrh2T</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Trump administration to require race data in c...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{'score': 0.762356, 'tone_id': 'analytical', ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>False</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>20</td>\n",
              "      <td>[\"https://twitter.com/primalpoly/status/125704...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>cognitivicta</td>\n",
              "      <td>Pro tip: if someone says they're 'very concern...</td>\n",
              "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
              "      <td>2020-05-03T23:43:29</td>\n",
              "      <td>1.588549e+09</td>\n",
              "      <td>1.257093e+18</td>\n",
              "      <td>/cognitivicta/status/1257093452036141057</td>\n",
              "      <td>2.434634e+07</td>\n",
              "      <td>Victor Rivera 🦸‍♂️ 🇵🇷😷</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pro tip: if someone says they're 'very concern...</td>\n",
              "      <td>[{'sentence_id': 0, 'text': \"Pro tip: if someo...</td>\n",
              "      <td>[{'score': 0.623434, 'tone_id': 'fear', 'tone_...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>False</td>\n",
              "      <td>[\"diagnostics\",\"COVID19\"]</td>\n",
              "      <td>[]</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>[\"https://apple.news/A45Zizks7SCeyG2TQGsKq7Q\"]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>DrDanKnecht</td>\n",
              "      <td>⁦@CVSHealth⁩ CMO Troy Brennan articulates the ...</td>\n",
              "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
              "      <td>2020-05-08T23:53:05</td>\n",
              "      <td>1.588982e+09</td>\n",
              "      <td>1.258908e+18</td>\n",
              "      <td>/DrDanKnecht/status/1258907809732837382</td>\n",
              "      <td>2.209482e+09</td>\n",
              "      <td>Dr Dan</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>⁦⁩ CMO Troy Brennan articulates the importance...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      has_media                                           hashtags  ... fear  anger\n",
              "452       False                                        [\"COVID19\"]  ...  NaN    NaN\n",
              "361       False                                                NaN  ...  NaN    NaN\n",
              "830       False                                                 []  ...  NaN    1.0\n",
              "1989       True  [\"CoronaVirus\",\"COVIDー19\",\"COVID19\",\"CoronaVir...  ...  NaN    NaN\n",
              "161        True        [\"Pence\",\"IDIOT\",\"COVID19\",\"TRUMPCLONEpic\"]  ...  NaN    1.0\n",
              "1320      False                                                 []  ...  NaN    NaN\n",
              "1122      False                                                 []  ...  NaN    1.0\n",
              "98        False                                                NaN  ...  NaN    NaN\n",
              "848       False                                                 []  ...  1.0    NaN\n",
              "160       False                          [\"diagnostics\",\"COVID19\"]  ...  NaN    NaN\n",
              "\n",
              "[10 rows x 76 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhkKreHjBkXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "787b81bc-00a2-45b1-de16-aae67c1e9aef"
      },
      "source": [
        "anger = df[['clean_text','anger']]\n",
        "anger.anger = [np.nan_to_num(x) for x in anger['anger']]\n",
        "anger = anger.astype({\"anger\": int})"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f41jcuzmh8-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Un-comment to predict tentative sentiment\n",
        "#tentative = df[['clean_text','tentative']]\n",
        "#tentative.tentative = [np.nan_to_num(x) for x in tentative['tentative']]\n",
        "#tentative = tentative.astype({\"tentative\": int})"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIaxUy7hiVaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Un-comment to predict analytical sentiment\n",
        "#analytical = df[['clean_text','analytical']]\n",
        "#analytical.analytical = [np.nan_to_num(x) for x in analytical['analytical']]\n",
        "#analytical = analytical.astype({\"analytical\": int})"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24SJzFlIiszM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Un-comment to predict sadness sentiment\n",
        "#sadness = df[['clean_text','sadness']]\n",
        "#sadness.sadness = [np.nan_to_num(x) for x in sadness['sadness']]\n",
        "#sadness = sadness.astype({\"sadness\": int})"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqzGzVKDivLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Un-comment to predict confident sentiment\n",
        "#confident = df[['clean_text','confident']]\n",
        "#confident.confident = [np.nan_to_num(x) for x in confident['confident']]\n",
        "#confident = confident.astype({\"confident\": int})"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS7Jzljni1qA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Un-comment to predict joy sentiment\n",
        "#joy = df[['clean_text','joy']]\n",
        "#joy.joy = [np.nan_to_num(x) for x in joy['joy']]\n",
        "#joy = joy.astype({\"joy\": int})"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJherbcki8gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Un-comment to predict fear sentiment\n",
        "#fear = df[['clean_text','fear']]\n",
        "#fear.fear = [np.nan_to_num(x) for x in fear['fear']]\n",
        "#fear = fear.astype({\"fear\": int})"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfs0yaLVCIc5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "f54ef235-54d9-4559-d4da-92bb04af5e8f"
      },
      "source": [
        "anger.sample(10)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>anger</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>Coronavirus, Vaccines and the Gates Foundation</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>What did the president know and when did he kn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>830</th>\n",
              "      <td>Holy shit the 5G coronavirus man</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>334</th>\n",
              "      <td>Dr. Dean Kindler joined  and  to discuss the i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>592</th>\n",
              "      <td>#Firstlady #MelaniaTrump turns 50 during #coro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1572</th>\n",
              "      <td>Only way to escape coronavirus</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1682</th>\n",
              "      <td>Most Americans can not, or simply refuse, to u...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>Trump said that playing \\nRussian Roulette mak...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1230</th>\n",
              "      <td>CORONAVIRUS TRACKER: After trending down for d...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>928</th>\n",
              "      <td>Updated BC Charts today, v. Justin McElroy CBC...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             clean_text  anger\n",
              "887     Coronavirus, Vaccines and the Gates Foundation       0\n",
              "92    What did the president know and when did he kn...      0\n",
              "830                   Holy shit the 5G coronavirus man       1\n",
              "334   Dr. Dean Kindler joined  and  to discuss the i...      0\n",
              "592   #Firstlady #MelaniaTrump turns 50 during #coro...      0\n",
              "1572                    Only way to escape coronavirus       0\n",
              "1682  Most Americans can not, or simply refuse, to u...      0\n",
              "477   Trump said that playing \\nRussian Roulette mak...      0\n",
              "1230  CORONAVIRUS TRACKER: After trending down for d...      0\n",
              "928   Updated BC Charts today, v. Justin McElroy CBC...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl-Rly_XMob0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = anger.clean_text.values\n",
        "labels = anger.anger.values\n",
        "\n",
        "#For other sentiments\n",
        "#sentences = tentative.clean_text.values\n",
        "#labels = tentative.tentative.values\n",
        "\n",
        "#sentences = analytical.clean_text.values\n",
        "#labels = analytical.analytical.values\n",
        "\n",
        "#sentences = sadness.clean_text.values\n",
        "#labels = sadness.sadness.values\n",
        "\n",
        "#sentences = confident.clean_text.values\n",
        "#labels = confident.confident.values\n",
        "\n",
        "#sentences = joy.clean_text.values\n",
        "#labels = joy.joy.values\n",
        "\n",
        "#sentences = fear.clean_text.values\n",
        "#labels = fear.fear.values"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ErbX-5iNhcb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74ae4231-0c1a-4f64-88b0-e361595d50a9"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry_n3qifNnXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2099226e-9751-428a-ff36-3c5331e0319f"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  #coronavirus #COVIDー19 #QuarantineActivities #FunniestTweets #MurderHornets #grizzlygators #CoronavirusPandemic Living my life one meme at a time!! pic.twitter.com/dCde5S5j1j\n",
            "Tokenized:  ['#', 'corona', '##virus', '#', 'co', '##vid', '##ー', '##19', '#', 'qu', '##aran', '##tine', '##act', '##iv', '##ities', '#', 'fun', '##nies', '##tt', '##wee', '##ts', '#', 'murder', '##horn', '##ets', '#', 'gr', '##izzly', '##gat', '##ors', '#', 'corona', '##virus', '##pan', '##de', '##mic', 'living', 'my', 'life', 'one', 'me', '##me', 'at', 'a', 'time', '!', '!', 'pic', '.', 'twitter', '.', 'com', '/', 'dc', '##de', '##5', '##s', '##5', '##j', '##1', '##j']\n",
            "Token IDs:  [1001, 21887, 23350, 1001, 2522, 17258, 30265, 16147, 1001, 24209, 20486, 10196, 18908, 12848, 6447, 1001, 4569, 15580, 4779, 28394, 3215, 1001, 4028, 9769, 8454, 1001, 24665, 29266, 20697, 5668, 1001, 21887, 23350, 9739, 3207, 7712, 2542, 2026, 2166, 2028, 2033, 4168, 2012, 1037, 2051, 999, 999, 27263, 1012, 10474, 1012, 4012, 1013, 5887, 3207, 2629, 2015, 2629, 3501, 2487, 3501]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6w8elb-58GJ",
        "colab_type": "text"
      },
      "source": [
        "## Sentences to IDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUBPDnPmNp9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "81214ce3-2e52-436c-d323-6b5567f173c3"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,  # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                              )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  #coronavirus #COVIDー19 #QuarantineActivities #FunniestTweets #MurderHornets #grizzlygators #CoronavirusPandemic Living my life one meme at a time!! pic.twitter.com/dCde5S5j1j\n",
            "Token IDs: [101, 1001, 21887, 23350, 1001, 2522, 17258, 30265, 16147, 1001, 24209, 20486, 10196, 18908, 12848, 6447, 1001, 4569, 15580, 4779, 28394, 3215, 1001, 4028, 9769, 8454, 1001, 24665, 29266, 20697, 5668, 1001, 21887, 23350, 9739, 3207, 7712, 2542, 2026, 2166, 2028, 2033, 4168, 2012, 1037, 2051, 999, 999, 27263, 1012, 10474, 1012, 4012, 1013, 5887, 3207, 2629, 2015, 2629, 3501, 2487, 3501, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYWEQk5nObfD",
        "colab_type": "text"
      },
      "source": [
        "## Padding & Truncating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9-KLCGmN6xB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c17a3b7c-2451-4b8e-f5af-1d989585b805"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPqkrKZkOCI_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "df51166d-2141-42ca-c938-06012b80cfed"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "MAX_LEN = 200\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGJG2x8EOidw",
        "colab_type": "text"
      },
      "source": [
        "## Attention Masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHZbsYeSOQoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUMyoyvHOo9H",
        "colab_type": "text"
      },
      "source": [
        "## Train/validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDHA8AcxOmK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                            random_state=1999, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                              random_state=1999, test_size=0.1)"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcVP5QjoPTd-",
        "colab_type": "text"
      },
      "source": [
        "## Converting to Pytorch Form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqXY3OPbPNSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf82JfnZPZOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v33eEBtpPg41",
        "colab_type": "text"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9TuWSa3PefG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95707e87-cd83-4fdb-845d-928ac3d4eb5b"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2,    \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnneE0Q4QFD_",
        "colab_type": "text"
      },
      "source": [
        "## Set Optimizer and Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecsQWY-XPnvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate \n",
        "                  eps = 1e-8 # args.adam_epsilon  \n",
        "                )\n"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8jM1XwrP9R4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                       num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                       num_training_steps = total_steps)"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7hl2qYVQKXC",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFFrud2NQBm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekKg0-2FQN_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxAt4zFNQRVG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "217fc577-69f8-4335-8a5f-8ce108f0fd0b"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 18\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:01:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:48.\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:01:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:48.\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:01:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:24.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:48.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ophvlNgxSuNT",
        "colab_type": "text"
      },
      "source": [
        "## Plot Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RQMCrQEQaCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "5cad1896-2cf3-47e5-c694-6fa2d0754ea8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY6SptSQZZ1l",
        "colab_type": "text"
      },
      "source": [
        "# Perform on Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KaJNZmgaOp4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9ea93329-44fb-44c1-a2cc-a9039c074e71"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/Test.csv\")\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "anger = df[['clean_text','anger']]\n",
        "\n",
        "#tentative = df[['clean_text','tentative']]\n",
        "#analytical = df[['clean_text','analytical']]\n",
        "#sadness = df[['clean_text','sadness']]\n",
        "#confident = df[['clean_text','confident']]\n",
        "#joy = df[['clean_text','joy']]\n",
        "#fear = df[['clean_text','fear']]"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TD4Z4NwaSf2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4570fd4d-6978-4803-9e54-b17e2f66c4ea"
      },
      "source": [
        "anger.anger = [np.nan_to_num(x) for x in anger['anger']]\n",
        "\n",
        "#tentative.tentative = [np.nan_to_num(x) for x in tentative['tentative']]\n",
        "#analytical.analytical = [np.nan_to_num(x) for x in analytical['analytical']]\n",
        "#sadness.sadness = [np.nan_to_num(x) for x in sadness['sadness']]\n",
        "#confident.confident = [np.nan_to_num(x) for x in confident['confident']]\n",
        "#joy.joy = [np.nan_to_num(x) for x in joy['joy']]\n",
        "#fear.fear = [np.nan_to_num(x) for x in fear['fear']]\n"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLpA6e1zaQIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anger = anger.astype({\"anger\": int})\n",
        "\n",
        "#tentative = tentative.astype({\"tentative\": int})\n",
        "#analytical = analytical.astype({\"analytical\": int})\n",
        "#sadness = sadness.astype({\"sadness\": int})\n",
        "#confident = confident.astype({\"confident\": int})\n",
        "#joy = joy.astype({\"joy\": int})\n",
        "#fear = fear.astype({\"fear\": int})"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4raPvx7Tg_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = anger.clean_text.values\n",
        "labels = anger.anger.values\n",
        "\n",
        "#sentences = tentative.clean_text.values\n",
        "#labels = tentative.tentative.values\n",
        "\n",
        "#sentences = analytical.clean_text.values\n",
        "#labels = analytical.analytical.values\n",
        "\n",
        "#sentences = sadness.clean_text.values\n",
        "#labels = sadness.sadness.values\n",
        "\n",
        "#sentences = confident.clean_text.values\n",
        "#labels = confident.confident.values\n",
        "\n",
        "#sentences = joy.clean_text.values\n",
        "#labels = joy.joy.values\n",
        "\n",
        "#sentences = fear.clean_text.values\n",
        "#labels = fear.fear.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 16  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey3Q3i1VaETC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "83d1cafd-a960-4259-ef5f-e9796f2b2f0e"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 500 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN37q1Hdav6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d0ae5a1-fb01-49c6-f9e7-e0be0ec3bfe7"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (anger.anger.sum(), len(anger.anger), (anger.anger.sum() / len(anger.anger) * 100.0)))\n",
        "\n",
        "#print('Positive samples: %d of %d (%.2f%%)' % (tentative.tentative.sum(), len(tentative.tentative), (tentative.tentative.sum() / len(tentative.tentative) * 100.0)))\n",
        "#print('Positive samples: %d of %d (%.2f%%)' % (analytical.analytical.sum(), len(analytical.analytical), (analytical.analytical.sum() / len(analytical.analytical) * 100.0)))\n",
        "#print('Positive samples: %d of %d (%.2f%%)' % (sadness.sadness.sum(), len(sadness.sadness), (sadness.sadness.sum() / len(sadness.sadness) * 100.0)))\n",
        "#print('Positive samples: %d of %d (%.2f%%)' % (confident.confident.sum(), len(confident.confident), (confident.confident.sum() / len(confident.confident) * 100.0)))\n",
        "#print('Positive samples: %d of %d (%.2f%%)' % (joy.joy.sum(), len(joy.joy), (joy.joy.sum() / len(joy.joy) * 100.0)))\n",
        "#print('Positive samples: %d of %d (%.2f%%)' % (fear.fear.sum(), len(fear.fear), (fear.fear.sum() / len(fear.fear) * 100.0)))"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 22 of 500 (4.40%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFvIcagGaz4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c55e1e19-c659-4174-dfb1-def2a97736cd"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U3k6ltEa65g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e325e2e-e801-4b9e-9382-27e4f33a9f75"
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwOqO5-hb5-L",
        "colab_type": "text"
      },
      "source": [
        "## Output Test Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_faiDsexc8vF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = [flat_true_labels, flat_predictions]"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "for1e9pddC8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = pd.DataFrame(frames)"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2kTQoE6dGCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "37cdce4e-ca57-40f8-845a-606fe78d2ff4"
      },
      "source": [
        "results = table.T\n",
        "results.columns =['True', 'Pred'] \n",
        "results.head()"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>True</th>\n",
              "      <th>Pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   True  Pred\n",
              "0     0     0\n",
              "1     0     0\n",
              "2     0     0\n",
              "3     0     0\n",
              "4     0     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_xHDbPAcQ_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table_to_save = anger.merge(results, left_index=True, right_index=True)\n",
        "\n",
        "#table_to_save = tentative.merge(results, left_index=True, right_index=True)\n",
        "#table_to_save = analytical.merge(results, left_index=True, right_index=True)\n",
        "#table_to_save = sadness.merge(results, left_index=True, right_index=True)\n",
        "#table_to_save = confident.merge(results, left_index=True, right_index=True)\n",
        "#table_to_save = joy.merge(results, left_index=True, right_index=True)\n",
        "#table_to_save = fear.merge(results, left_index=True, right_index=True)"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZA_4lcQb-qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table_to_save.to_csv('testing_data_results.csv')"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBKuOQ8_eY4F",
        "colab_type": "text"
      },
      "source": [
        "# Label Other Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2O4kaGs8e8iL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f2195afa-e971-4b0e-84a9-cf5ccd955d15"
      },
      "source": [
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/DS_week8/BERT/covid2019_BERT_cleans.csv\")\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of unlabeled sentences: {:,}\\n'.format(df.shape[0]))\n",
        "analytical_1 = df[['clean_text']]"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unlabeled sentences: 3,107\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ADDNg_0ezb2",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = analytical_1.clean_text.values\n",
        "# labels = sadness.sadness.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "# prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 16  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pMbfMQ2Fezb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "12d08f09-e771-4523-dd52-576c607b1046"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions  = []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  # label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  # true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 3,107 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpl0hjpqef1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEykTu38k0GJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table1 = pd.DataFrame(flat_predictions)\n",
        "table1.columns = ['pred_anger']\n",
        "\n",
        "#table1.columns = ['pred_tentative']\n",
        "#table1.columns = ['pred_analytical']\n",
        "#table1.columns = ['pred_sadness']\n",
        "#table1.columns = ['pred_confident']\n",
        "#table1.columns = ['pred_joy']\n",
        "#table1.columns = ['pred_fear']\n",
        "\n",
        "\n",
        "df1 = df[['text', 'timestamp', 'clean_text']]"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOdrHDAa10-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_output1 = df1.merge(table1,left_index=True, right_index=True)\n",
        "pred_output1.to_csv('data_test_0324_0408_anger_lite.csv')\n",
        "\n",
        "#pred_output1.to_csv('data_test_0324_0408_tentative_lite.csv')\n",
        "#pred_output1.to_csv('data_test_0324_0408_analytical_lite.csv')\n",
        "#pred_output1.to_csv('data_test_0324_0408_sadness_lite.csv')\n",
        "#pred_output1.to_csv('data_test_0324_0408_confident_lite.csv')\n",
        "#pred_output1.to_csv('data_test_0324_0408_joy_lite.csv')\n",
        "#pred_output1.to_csv('data_test_0324_0408_fear_lite.csv')"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1z6Onqn-fGj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e7a9499-185c-4d3a-cb40-ab37c5a951c4"
      },
      "source": [
        "len(pred_output1)"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    }
  ]
}